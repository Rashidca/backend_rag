
# ğŸ“˜ Camera Manual RAG â€” Local LLM (Ollama + Metadata-Aware Retrieval)

This project implements an **advanced Retrieval-Augmented Generation (RAG)** system that answers questions from a **technical PDF manual** with **chapter-aware metadata filtering**.
Unlike standard RAG pipelines that search blindly across the whole document, this system detects the **relevant chapter first** and retrieves only those chunks â€” delivering far more accurate answers.

ğŸ§  **Embeddings:** Hugging Face BGE Small
ğŸ¤– **LLM:** **Ollama â€” Qwen 2.5 3B Instruct (local CPU inference)**
ğŸ“„ **Dataset:** Camera user manual (but you can replace with ANY manual)

---

## ğŸš€ Features

* Chapter-aware vector search (metadata filtering)
* Rejects hallucination â€” answers **only from provided chunks**
* Streamlit UI for user queries
* FastAPI backend
* Performance logging:

  * Retrieval latency
  * Generation latency
* Works fully **offline** once dependencies + Ollama model installed

---

## ğŸ“º Demo Video

ğŸ“Œ *YouTube Link:*
â–¶ï¸ [*(paste your link here)*](https://youtu.be/zT2QTyO-Ezw)

---

## ğŸ“‚ Project Structure

```
rag-manual/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ manual.pdf              â† place your PDF here
â”‚
â”œâ”€â”€ ingestion.py                â† one-time ingestion pipeline
â”œâ”€â”€ retriever.py                â† hybrid semantic + chapter filtering
â”œâ”€â”€ models.py                   â† embeddings + local LLM caller
â”œâ”€â”€ query_api.py                â† FastAPI runtime service
â”œâ”€â”€ chat_ui.py                  â† Streamlit UI
â”‚
â”œâ”€â”€ requirements.txt
```



## ğŸ›  Setup

### 1ï¸âƒ£ Install Ollama

[https://ollama.com/](https://ollama.com/)

Then pull the model:

```bash
ollama pull qwen2.5:3b-instruct
```

### 2ï¸âƒ£ Create and activate virtual environment

```bash
python -m venv venv
```

```bash
# Windows
venv\Scripts\activate
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ“¥ Ingest the Manual

Add your manual here:

```
data/manual.pdf
```

Then run:

```bash
python ingestion.py
```

This creates the Chroma vector database.

---

## â–¶ï¸ Run Backend API

```bash
python query_api.py
```

Backend runs at:

```
http://localhost:8000/query
```

---

## ğŸ’» Launch UI

```bash
streamlit run chat_ui.py
```

Ask your questions â€” example:

```
What are the steps in the Quick Start Guide for the D3300?
```

If the answer isn't present in the retrieved context, the system replies:

```
"I don't know based on this manual."
```

---

## âš¡ Performance on My Machine (No GPU)

| Component  | Latency                     |
| ---------- | --------------------------- |
| Retrieval  | â³ ~3â€“4 seconds              |
| Generation | â³ ~20-30 seconds (CPU only) |

Generation can be much faster with GPU models.

---

## ğŸ–¼ Screenshots 

```
/assets/streamlit.png
/assets/logs.png
/assets/context.png
```

---

## ğŸ”® Future Improvements

* GPU inference for faster generation
* OCR for manuals containing images
* Support for multiple manuals
* Dashboard for testing multiple models

---

## â­ Final Notes

* You can use *any* manual â€” just rename it to **manual.pdf** and place it in `/data`
* Ollama must be running while using the system
* Run ingestion only once unless you change the PDF

